{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c2846-7108-4b7c-a8a8-6ef3e14567f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from datasets import load_image_data \n",
    "from models import ImageCaptioningModel, TextGenerationModel  \n",
    "from utils import save_model, load_model \n",
    "from eval import evaluate_captioning  \n",
    "\n",
    "class InteractiveImageCaptioner:\n",
    "    def __init__(self, \n",
    "                 caption_model_name=\"Salesforce/blip-image-captioning-base\",\n",
    "                 text_gen_model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initialize image captioning and text generation models\n",
    "\n",
    "        Args:\n",
    "            caption_model_name (str): Hugging Face model for initial image captioning\n",
    "            text_gen_model_name (str): Hugging Face model for text modification\n",
    "        \"\"\"\n",
    "        # Image Captioning Model\n",
    "        self.caption_model = ImageCaptioningModel(caption_model_name)\n",
    "\n",
    "        # Text Generation Model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_gen_model_name)\n",
    "        self.text_generator = AutoModelForCausalLM.from_pretrained(text_gen_model_name)\n",
    "    \n",
    "    def generate_initial_caption(self, image_path):\n",
    "        \"\"\"\n",
    "        Generate initial caption for the image\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to the image file\n",
    "\n",
    "        Returns:\n",
    "            str: Initial image caption\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and preprocess the image using dataset handling function\n",
    "            image_data = load_image_data(image_path)\n",
    "            \n",
    "            # Generate caption using the caption model\n",
    "            caption = self.caption_model.generate_caption(image_data)\n",
    "            \n",
    "            # Return the caption\n",
    "            return caption if caption else \"No caption generated\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating initial caption: {e}\")\n",
    "            return \"Caption generation failed\"\n",
    "    \n",
    "    def modify_caption_with_emotion(self, initial_caption, user_emotion):\n",
    "        \"\"\"\n",
    "        Modify caption based on user's emotional input\n",
    "\n",
    "        Args:\n",
    "            initial_caption (str): Original image caption\n",
    "            user_emotion (str): User's desired emotion or description\n",
    "\n",
    "        Returns:\n",
    "            str: Modified caption\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construct prompt for text generation\n",
    "            prompt = f\"The image shows {initial_caption}. Describe it with a {user_emotion} tone: \"\n",
    "            \n",
    "            # Encode the prompt\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "            \n",
    "            # Generate modified text\n",
    "            output = self.text_generator.generate(\n",
    "                input_ids, \n",
    "                max_length=100, \n",
    "                num_return_sequences=1, \n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            modified_caption = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            return modified_caption\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying caption: {e}\")\n",
    "            return initial_caption\n",
    "\n",
    "def main():\n",
    "    # Path to your image (replace with actual path)\n",
    "    IMAGE_PATH = '/Users/rohitharavindramyla/Desktop/CSCI2470_Project/coco2017/train2017/000000000009.jpg'\n",
    "    \n",
    "    # Initialize the interactive captioner\n",
    "    captioner = InteractiveImageCaptioner()\n",
    "    \n",
    "    # Generate initial caption\n",
    "    initial_caption = captioner.generate_initial_caption(IMAGE_PATH)\n",
    "    print(\"Initial Caption:\", initial_caption)\n",
    "    \n",
    "    # Interactive user input for emotion\n",
    "    print(\"\\n--- Interactive Image Captioning ---\")\n",
    "    print(\"Original Caption:\", initial_caption)\n",
    "    \n",
    "    # Get user emotion input\n",
    "    user_emotion = input(\"Enter an emotion or description to modify the caption (e.g., 'sad', 'excited', 'mysterious'): \")\n",
    "    \n",
    "    # Modify caption based on user input\n",
    "    modified_caption = captioner.modify_caption_with_emotion(initial_caption, user_emotion)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(\"Initial Caption:\", initial_caption)\n",
    "    print(f\"Caption with {user_emotion} tone:\", modified_caption)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8bf76-fe90-4dfb-a0c1-775d950fcea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI2470",
   "language": "python",
   "name": "csci2470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
