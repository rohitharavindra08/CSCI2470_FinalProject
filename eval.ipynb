{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f0f73-85ff-4d32-bc3d-b9b88c2cc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Define parameters for evaluation.\n",
    "\n",
    "    :return: arguments\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Evaluation')\n",
    "\n",
    "    parser.add_argument('--checkpoint_folder', '-cf', default='/Users/rohitharavindramyla/Desktop/CSCI2470_Project/checkpoints/',\n",
    "                    help='path to checkpoint')\n",
    "    parser.add_argument('--dataset', '-d', default='flickr8k', help='dataset')\n",
    "    parser.add_argument('--beam_size', '-b', default=5, type=int, help='beam size for beam search')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def evaluate(args):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "\n",
    "    :param beam_size: beam size at which to generate captions for evaluation\n",
    "    :return: BLEU scores\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    checkpoint = torch.load(os.path.join(args.checkpoint_folder,\n",
    "                                         'BEST_checkpoint_{:s}_5_cap_per_img_5_min_word_freq.pth.tar'.format(\n",
    "                                             args.dataset)))\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder = decoder.to(device)\n",
    "    decoder.eval()\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    # Load word map (word2ix)\n",
    "    word_map_file = os.path.join('/Users/rohitharavindramyla/Desktop/CSCI2470_Project/wordmaps',\n",
    "                             'WORDMAP_{:s}_5_cap_per_img_5_min_word_freq.json'.format(args.dataset))\n",
    "    with open(word_map_file, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Normalization transform\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # DataLoader\n",
    "    data_folder = '/Users/rohitharavindramyla/Desktop/CSCI2470_Project/data'  # folder with data files saved by create_input_files.py\n",
    "    data_name = '{:s}_5_cap_per_img_5_min_word_freq'.format(args.dataset)  # base name shared by data files\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
    "        batch_size=1, shuffle=True, pin_memory=False)\n",
    "\n",
    "    # TODO: Batched Beam Search\n",
    "\n",
    "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "\n",
    "    # For each image\n",
    "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
    "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(args.beam_size))):\n",
    "\n",
    "        k = args.beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        image = image.to(device)  # (1, 3, 256, 256)\n",
    "\n",
    "        # Encode\n",
    "        try:\n",
    "            if decoder.adaptive_att:\n",
    "                encoder_out, v_g = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "\n",
    "            else:\n",
    "                encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "\n",
    "        except AttributeError:\n",
    "            encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "\n",
    "        enc_image_size = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(3)\n",
    "\n",
    "        # Flatten encoding\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "            try:\n",
    "\n",
    "                if decoder.adaptive_att:\n",
    "\n",
    "                    g_t = decoder.sigmoid(decoder.affine_embed(embeddings) + decoder.affine_decoder(h))\n",
    "                    s_t = g_t * torch.tanh(c)\n",
    "\n",
    "                    h, c = decoder.decode_step_adaptive(torch.cat([embeddings, v_g.expand_as(embeddings)], dim=1), (h, c))  # (batch_size_t, decoder_dim)\n",
    "\n",
    "                    attention_weighted_encoding, alpha = decoder.adaptive_attention(encoder_out, h, s_t)\n",
    "\n",
    "                    scores = decoder.fc(h) + decoder.fc_encoder(attention_weighted_encoding)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "                    gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "                    awe = gate * awe\n",
    "\n",
    "                    h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "                    scores = decoder.fc(h)  # (s, vocab_size)\n",
    "\n",
    "            except AttributeError:\n",
    "\n",
    "                awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "                awe = gate * awe\n",
    "\n",
    "                h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "                scores = decoder.fc(h)  # (s, vocab_size)\n",
    "\n",
    "\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
    "\n",
    "        assert len(references) == len(hypotheses)\n",
    "\n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0) )\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    return bleu1 ,bleu2, bleu3, bleu4\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = get_args()\n",
    "\n",
    "    bleu1, bleu2, bleu3, bleu4 = evaluate(args)\n",
    "    print(\"\\nBLEU-1 score @ beam size of %d is %.4f.\" % (args.beam_size, bleu1))\n",
    "    print(\"\\nBLEU-2 score @ beam size of %d is %.4f.\" % (args.beam_size, bleu2))\n",
    "    print(\"\\nBLEU-3 score @ beam size of %d is %.4f.\" % (args.beam_size, bleu3))\n",
    "    print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (args.beam_size, bleu4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199fd0a-4716-4045-8e01-588cd740bb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI2470",
   "language": "python",
   "name": "csci2470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
